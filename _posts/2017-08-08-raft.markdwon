---
layout:     post
title:      "分布式一致性算法-Raft"
subtitle:   " \"相比于Paxos更易理解的分布式系统一致性算法--Raft\""
date:       2017-08-08 17：08
header-img: "img/post-bg-unix-linux.jpg" 
tags:
    - Raft
    - algorithm
---

### 1. 先介绍一下--Pasox

Paxos算法是莱斯利·兰伯特（Leslie Lamport，就是 LaTeX 中的"La"，此人现在在微软研究院）于1990年提出的一种基于消息传递的一致性算法。  
Leslie Lamport本人认为Paxos算法是简单的，但是广大群众公认Paxos算法非常复杂。Google的分布式锁系统Chubby作为Paxos实现曾经遭遇到很多坑。

### 2. Raft简介

来自Stanford的新的分布式协议研究称为Raft，它是一个为真实世界应用建立的协议，主要注重协议的落地性和可理解性。

在了解Raft之前，我们先了解Consensus一致性这个概念，它是指多个服务器在状态达成一致，但是在一个分布式系统中，因为各种意外可能，有的服务器可能会崩溃或变得不可靠，它就不能和其他服务器达成一致状态。这样就需要一种Consensus协议，一致性协议是为了确保容错性，也就是即使系统中有一两个服务器当机，也不会影响其处理过程。

为了以容错方式达成一致，我们不可能要求所有服务器100%都达成一致状态，只要超过半数的大多数服务器达成一致就可以了，假设有N台服务器，N/2 +1 就超过半数，代表大多数了。

Paxos和Raft都是为了实现Consensus一致性这个目标，这个过程如同选举一样，参选者需要说服大多数选民(服务器)投票给他，一旦选定后就跟随其操作。Paxos和Raft的区别在于选举的具体过程不同。

### 3. Raft角色

在Raft中，任何时候一个服务器可以扮演下面3种角色之一：

- Leader: 处理所有客户端交互，日志复制等，一般一次只有一个Leader
- Follower: 类似选民，完全被动
- Candidate: 候选人，竞选新Leader时的角色

### 4. Raft选举过程说明

在极简的思维下，一个最小的 Raft 民主集群需要三个参与者（如下图：A、B、C），这样才可能投出多数票。初始状态 ABC 都是 Follower，然后发起选举这时有三种可能情形发生。下图中前二种都能选出 Leader，第三种则表明本轮投票无效（Split Votes），每方都投给了自己，结果没有任何一方获得多数票。之后每个参与方随机休息一阵（Election Timeout）重新发起投票直到一方获得多数票。这里的关键就是随机 timeout，最先从 timeout 中恢复发起投票的一方向还在 timeout 中的另外两方请求投票，这时它们就只能投给对方了，很快达成一致。

![image](http://blog.lpc-win32.com/img/2017-08-08/raft-01.png)

### 5. Leader节点对一致性的影响

Raft 协议强依赖 Leader 节点的可用性来确保集群数据的一致性。数据的流向只能从 Leader 节点向 Follower 节点转移。当 Client 向集群 Leader 节点提交数据后，Leader 节点接收到的数据处于未提交状态（Uncommitted），接着 Leader 节点会并发向所有 Follower 节点复制数据并等待接收响应，确保至少集群中超过半数节点已接收到数据后再向 Client 确认数据已接收。一旦向 Client 发出数据接收 Ack 响应后，表明此时数据状态进入已提交（Committed），Leader 节点再向 Follower 节点发通知告知该数据状态已提交。

#### 5.1 数据到达Leader节点前

在这个阶段Leader挂掉不影响一致性

![image](http://blog.lpc-win32.com/img/2017-08-08/raft-02.png)

#### 5.2 数据到达Leader节点，但未复制到Follower节点

这个阶段 Leader 挂掉，数据属于未提交状态，Client 不会收到 Ack 会认为超时失败可安全发起重试。Follower 节点上没有该数据，重新选主后 Client 重试重新提交可成功。原来的 Leader 节点恢复后作为 Follower 加入集群重新从当前任期的新 Leader 处同步数据，强制保持和 Leader 数据一致。

![image](http://blog.lpc-win32.com/img/2017-08-08/raft-03.png)

#### 5.3 数据到达Leader节点，成功复制到Follower所有节点，但还未向Leader响应接收

这个阶段 Leader 挂掉，虽然数据在 Follower 节点处于未提交状态（Uncommitted）但保持一致，重新选出 Leader 后可完成数据提交，此时 Client 由于不知到底提交成功没有，可重试提交。针对这种情况 Raft 要求 RPC 请求实现幂等性，也就是要实现内部去重机制。

![image](http://blog.lpc-win32.com/img/2017-08-08/raft-04.png)

#### 5.4 数据到达Leader节点，成功复制到Follower部分节点，但还未向Leader响应接收

这个阶段 Leader 挂掉，数据在 Follower 节点处于未提交状态（Uncommitted）且不一致，Raft 协议要求投票只能投给拥有最新数据的节点。所以拥有最新数据的节点会被选为 Leader 再强制同步数据到 Follower，数据不会丢失并最终一致。

![image](http://blog.lpc-win32.com/img/2017-08-08/raft-05.png)

#### 5.5 数据到达Leader节点，成功复制到Follower所有或多数节点，数据在Leader处于已提交状态，但在Follower处于未提交状态

这个阶段 Leader 挂掉，重新选出新 Leader 后的处理流程和阶段 3 一样。

![image](http://blog.lpc-win32.com/img/2017-08-08/raft-06.png)

#### 5.6 数据到达Leader节点，成功复制到Follower所有或多数节点，数据在所有节点都处于已提交状态，但还未响应Client

这个阶段 Leader 挂掉，Cluster 内部数据其实已经是一致的，Client 重复重试基于幂等策略对一致性无影响。

![image](http://blog.lpc-win32.com/img/2017-08-08/raft-07.png)

#### 5.7 网络分区导致的脑裂情况，出现双Leader

网络分区将原先的 Leader 节点和 Follower 节点分隔开，Follower 收不到 Leader 的心跳将发起选举产生新的 Leader。这时就产生了双 Leader，原先的 Leader 独自在一个区，向它提交数据不可能复制到多数节点所以永远提交不成功。向新的 Leader 提交数据可以提交成功，网络恢复后旧的 Leader 发现集群中有更新任期（Term）的新 Leader 则自动降级为 Follower 并从新 Leader 处同步数据达成集群数据一致。

![image](http://blog.lpc-win32.com/img/2017-08-08/raft-08.png)

### 6. 总结

更直观的教程见：[英文动画演示Raft](http://thesecretlivesofdata.com/raft/)
